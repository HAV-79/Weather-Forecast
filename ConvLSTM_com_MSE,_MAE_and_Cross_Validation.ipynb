{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPi/sI98wsJfdYWETSN6Lr3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAV-79/Weather-Forecast/blob/main/ConvLSTM_com_MSE%2C_MAE_and_Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17nyg293bwgS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/MyDrive/dataset Next Frame/conv_lstm_data_1000_128x128_20f.npy'\n",
        "\n",
        "# Download and load the dataset.\n",
        "fpath = path\n",
        "dataset = np.load(fpath)\n",
        "\n",
        "# Swap the axes representing the number of frames and number of data samples.\n",
        "dataset = np.swapaxes(dataset, 0, 1)\n",
        "# We'll pick out 1000 of the 10000 total examples and use those.\n",
        "dataset = dataset[:1000, ...]\n",
        "# Add a channel dimension since the images are grayscale.\n",
        "# dataset = np.expand_dims(dataset, axis=-1)\n",
        "\n",
        "# Normalize the data to the 0-1 range.\n",
        "dataset = dataset / 255\n",
        "\n",
        "# Define a helper function to shift the frames.\n",
        "def create_shifted_frames(data):\n",
        "    x = data[:, 0 : data.shape[1] - 1, :, :]\n",
        "    y = data[:, 1 : data.shape[1], :, :]\n",
        "    return x, y\n",
        "\n",
        "# Parameters\n",
        "epochs = 40\n",
        "batch_size = 2\n",
        "n_splits = 2  # Number of folds for K-fold validation\n",
        "\n",
        "# Create KFold object\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Placeholder for history data\n",
        "histories = []\n",
        "\n",
        "for train_index, val_index in kf.split(dataset):\n",
        "    train_dataset = dataset[train_index]\n",
        "    val_dataset = dataset[val_index]\n",
        "\n",
        "    # Apply the processing function to the datasets.\n",
        "    x_train, y_train = create_shifted_frames(train_dataset)\n",
        "    x_val, y_val = create_shifted_frames(val_dataset)\n",
        "\n",
        "    # Construct the input layer with no definite frame size.\n",
        "    inp = layers.Input(shape=(None, *x_train.shape[2:]))\n",
        "\n",
        "    # Construct ConvLSTM2D and Conv3D layers.\n",
        "    x = layers.ConvLSTM2D(\n",
        "        filters=64,\n",
        "        kernel_size=(5, 5),\n",
        "        padding=\"same\",\n",
        "        return_sequences=True,\n",
        "        activation=\"relu\",\n",
        "    )(inp)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ConvLSTM2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        padding=\"same\",\n",
        "        return_sequences=True,\n",
        "        activation=\"relu\",\n",
        "    )(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ConvLSTM2D(\n",
        "        filters=64,\n",
        "        kernel_size=(1, 1),\n",
        "        padding=\"same\",\n",
        "        return_sequences=True,\n",
        "        activation=\"relu\",\n",
        "    )(x)\n",
        "    x = layers.Conv3D(\n",
        "        filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
        "    )(x)\n",
        "\n",
        "    # Build and compile the model.\n",
        "    model = keras.models.Model(inp, x)\n",
        "    model.compile(\n",
        "        loss='mse',  # Mean Squared Error as loss function\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "        metrics=['mae', 'mse']  # Mean Absolute Error and Mean Squared Error as metrics\n",
        "    )\n",
        "    model.summary()\n",
        "\n",
        "    # Define callbacks for early stopping and learning rate reduction.\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "\n",
        "    # Fit the model to the training data and get the history.\n",
        "    history = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "    )\n",
        "\n",
        "    # Append history for later analysis\n",
        "    histories.append(history)\n",
        "\n",
        "# Create a directory to save the model if it doesn't exist.\n",
        "output_directory = \"/content/drive/MyDrive/Modelos\"\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Save the trained model\n",
        "model.save(os.path.join(output_directory, 'novo_modelo_1000_40pc_nextframe.h5'))\n",
        "\n",
        "# Plot combined training & validation loss values from all folds\n",
        "for i, history in enumerate(histories):\n",
        "    plt.plot(history.history['loss'], label=f'Train Loss Fold {i+1}')\n",
        "    plt.plot(history.history['val_loss'], label=f'Validation Loss Fold {i+1}')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot combined training & validation MAE values from all folds\n",
        "for i, history in enumerate(histories):\n",
        "    plt.plot(history.history['mae'], label=f'Train MAE Fold {i+1}')\n",
        "    plt.plot(history.history['val_mae'], label=f'Validation MAE Fold {i+1}')\n",
        "plt.title('Model Mean Absolute Error')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot combined training & validation MSE values from all folds\n",
        "for i, history in enumerate(histories):\n",
        "    plt.plot(history.history['mse'], label=f'Train MSE Fold {i+1}')\n",
        "    plt.plot(history.history['val_mse'], label=f'Validation MSE Fold {i+1}')\n",
        "plt.title('Model Mean Squared Error')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frame Prediction Visualizations - Model trained in 20 epochs."
      ],
      "metadata": {
        "id": "CoZyu9_rckgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Caminho para o dataset\n",
        "path = '/content/drive/MyDrive/dataset Next Frame/conv_lstm_data_20_128x128_20f.npy'\n",
        "\n",
        "# Carregar o dataset\n",
        "fpath = path\n",
        "dataset = np.load(fpath)\n",
        "\n",
        "# Trocar os eixos representando o número de frames e o número de imagens\n",
        "dataset = np.swapaxes(dataset, 0, 1)\n",
        "\n",
        "# Vamos escolher 20 imagens\n",
        "dataset = dataset[:20, ...]\n",
        "\n",
        "# Normalizar os dados para a faixa 0-1\n",
        "dataset = dataset / 255.0\n",
        "\n",
        "# Função auxiliar para deslocar os frames\n",
        "def create_shifted_frames(data):\n",
        "    x = data[:, :-1, :, :]\n",
        "    y = data[:, 1:, :, :]\n",
        "    return x, y\n",
        "\n",
        "# Criar dataset de treino e validação (por exemplo, 80% treino e 20% validação)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset = dataset[:train_size]\n",
        "val_dataset = dataset[train_size:]\n",
        "\n",
        "# Carregar o modelo salvo\n",
        "model_path = '/content/drive/MyDrive/Modelos/modelo_1000_40epc_nextframe_5.h5'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "\n",
        "\n",
        "# Selecionar um exemplo aleatório do dataset de validação\n",
        "example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n",
        "\n",
        "# Escolher os primeiros/últimos dez frames do exemplo\n",
        "frames = example[:10, ...]\n",
        "original_frames = example[10:, ...]\n",
        "\n",
        "# Prever um novo conjunto de 10 frames\n",
        "for _ in range(10):\n",
        "    # Extrair a predição do modelo e pós-processar\n",
        "    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "    new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "    # Estender o conjunto de frames de predição\n",
        "    frames = np.concatenate((frames, predicted_frame), axis=0)\n",
        "\n",
        "# Construir uma figura para os frames originais e novos\n",
        "fig, axes = plt.subplots(2, 10, figsize=(40, 10))\n",
        "\n",
        "# Plotar os frames originais\n",
        "for idx, ax in enumerate(axes[0]):\n",
        "    ax.imshow(np.squeeze(original_frames[idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Plotar os novos frames\n",
        "new_frames = frames[10:, ...]\n",
        "for idx, ax in enumerate(axes[1]):\n",
        "    ax.imshow(np.squeeze(new_frames[idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Mostrar a figura\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HHUfVs4Vcmsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Videos Forecast saving in Gifs\n"
      ],
      "metadata": {
        "id": "cX9eqC8ndZuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from ipywidgets import HBox, widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Carregar o modelo salvo\n",
        "model_path = '/content/drive/MyDrive/Modelos/modelo_1000_40epc_nextframe_5.h5'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Crie um diretório para salvar os GIFs se ele não existir\n",
        "output_directory = \"Predicted_1000i\"\n",
        "if not os.path.exists(output_directory):\n",
        "    os.makedirs(output_directory)\n",
        "\n",
        "# Suponha que val_dataset já esteja definido e carregado corretamente\n",
        "# Exemplo: val_dataset = np.load('val_dataset.npy')\n",
        "# (Você deve ajustar esta linha conforme necessário para carregar seu conjunto de dados de validação)\n",
        "\n",
        "# Selecione alguns exemplos aleatórios do conjunto de dados de validação.\n",
        "examples = val_dataset[np.random.choice(range(len(val_dataset)), size=5)]\n",
        "\n",
        "# Iterar sobre os exemplos e prever os frames.\n",
        "for idx, example in enumerate(examples):\n",
        "    # Escolha os primeiros/últimos dez frames do exemplo.\n",
        "    frames = example[:10, ...]\n",
        "    original_frames = example[10:, ...]\n",
        "    new_predictions = np.zeros(shape=(10, *frames[0].shape))\n",
        "\n",
        "    # Prever um novo conjunto de 10 frames.\n",
        "    for i in range(10):\n",
        "        # Extraia a previsão do modelo e pós-processe-a.\n",
        "        frames = example[: 10 + i + 1, ...]\n",
        "        new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "        new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "        predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "        # Estenda o conjunto de frames de previsão.\n",
        "        new_predictions[i] = predicted_frame\n",
        "\n",
        "    # Salvar GIFs para cada uma das imagens de verdade/predição.\n",
        "    for set_name, frame_set in zip([\"truth\", \"prediction\"], [original_frames, new_predictions]):\n",
        "        # Construir um GIF a partir dos frames de vídeo selecionados.\n",
        "        current_frames = np.squeeze(frame_set)\n",
        "        if current_frames.ndim == 3:  # Caso haja apenas uma dimensão de cor\n",
        "            current_frames = current_frames[..., np.newaxis] * np.ones(3)\n",
        "        current_frames = (current_frames * 255).astype(np.uint8)\n",
        "\n",
        "        # Salvar o GIF como um arquivo.\n",
        "        gif_filename = os.path.join(output_directory, f\"example_{idx}_set_{set_name}.gif\")\n",
        "        imageio.mimsave(gif_filename, current_frames, \"GIF\", duration=0.2)\n",
        "\n",
        "# Exibir os vídeos.\n",
        "print(\"Truth\\tPrediction\")\n",
        "for i in range(len(examples)):\n",
        "    # Construir e exibir um `HBox` com a verdade e a previsão.\n",
        "    truth_gif_path = os.path.join(output_directory, f\"example_{i}_set_truth.gif\")\n",
        "    prediction_gif_path = os.path.join(output_directory, f\"example_{i}_set_prediction.gif\")\n",
        "    box = HBox(\n",
        "        [\n",
        "            widgets.Image(value=open(truth_gif_path, \"rb\").read(), format='gif'),\n",
        "            widgets.Image(value=open(prediction_gif_path, \"rb\").read(), format='gif'),\n",
        "        ]\n",
        "    )\n",
        "    display(box)\n"
      ],
      "metadata": {
        "id": "5M1p6YCMdaux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Make the prediction and save it in png format in a folder**"
      ],
      "metadata": {
        "id": "S2-MZ00JFfwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Montar o Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Caminho para o dataset\n",
        "path = '/content/drive/MyDrive/dataset Next Frame/conv_lstm_data_20_128x128_20f.npy'\n",
        "\n",
        "# Carregar o dataset\n",
        "fpath = path\n",
        "dataset = np.load(fpath)\n",
        "\n",
        "# Trocar os eixos representando o número de frames e o número de exemplos de dados\n",
        "dataset = np.swapaxes(dataset, 0, 1)\n",
        "# Vamos escolher 200 exemplos do total de exemplos\n",
        "dataset = dataset[:10, ...]\n",
        "\n",
        "# Normalizar os dados para a faixa 0-1\n",
        "dataset = dataset / 255.0\n",
        "\n",
        "# Função auxiliar para deslocar os frames\n",
        "def create_shifted_frames(data):\n",
        "    x = data[:, :-1, :, :]\n",
        "    y = data[:, 1:, :, :]\n",
        "    return x, y\n",
        "\n",
        "# Criar dataset de treino e validação (por exemplo, 80% treino e 20% validação)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "train_dataset = dataset[:train_size]\n",
        "val_dataset = dataset[train_size:]\n",
        "\n",
        "# Carregar o modelo salvo\n",
        "model_path = '/content/drive/MyDrive/Modelos/modelo_600i_2opc_2kfolder_nextframe.h5'\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Selecionar um exemplo aleatório do dataset de validação\n",
        "example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n",
        "\n",
        "# Escolher os primeiros/últimos dez frames do exemplo\n",
        "frames = example[:10, ...]\n",
        "original_frames = example[10:, ...]\n",
        "\n",
        "# Definir a pasta onde os novos frames serão salvos\n",
        "output_dir = '/content/drive/MyDrive/PredictedFrames1000-20epoc'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Função para salvar um frame como PNG\n",
        "def save_frame_as_png(frame, filename):\n",
        "    frame = (frame * 255).astype(np.uint8)  # Convertendo de volta para 0-255\n",
        "    image = Image.fromarray(frame.squeeze(), 'L')  # 'L' para imagens em escala de cinza\n",
        "    image.save(filename)\n",
        "\n",
        "# Prever um novo conjunto de 10 frames\n",
        "for i in range(10):\n",
        "    # Extrair a predição do modelo e pós-processar\n",
        "    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
        "    new_prediction = np.squeeze(new_prediction, axis=0)\n",
        "    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
        "\n",
        "    # Estender o conjunto de frames de predição\n",
        "    frames = np.concatenate((frames, predicted_frame), axis=0)\n",
        "\n",
        "    # Salvar o frame predito como PNG\n",
        "    save_frame_as_png(predicted_frame, os.path.join(output_dir, f'predicted_frame_{i+1}.png'))\n",
        "\n",
        "# Construir uma figura para os frames originais e novos\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 4))\n",
        "\n",
        "# Plotar os frames originais\n",
        "for idx, ax in enumerate(axes[0]):\n",
        "    ax.imshow(np.squeeze(original_frames[idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Plotar os novos frames\n",
        "new_frames = frames[5:, ...]\n",
        "for idx, ax in enumerate(axes[1]):\n",
        "    ax.imshow(np.squeeze(new_frames[idx]), cmap=\"gray\")\n",
        "    ax.set_title(f\"Frame {idx + 11}\")\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "# Mostrar a figura\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zdl-Yv7AFd6y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}