{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLh39xbFYofihA7fmRE6nA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HAV-79/Weather-Forecast/blob/main/Classifica%C3%A7%C3%A3o_do_tempo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funcionando 100% no VSCode"
      ],
      "metadata": {
        "id": "ogLJcZwwJJkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from keras import Sequential\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, img_to_array, load_img\n",
        "from tensorflow.python.client import device_lib\n",
        "from tensorflow.keras.utils import model_to_dot\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "from IPython.display import SVG\n",
        "from PIL import Image\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from os import mkdir\n",
        "\n",
        "import scipy\n",
        "\n",
        "tf.test.gpu_device_name()\n",
        "device_lib.list_local_devices()\n",
        "\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "   print('GPU disponível')\n",
        "\n",
        "else:\n",
        "   print('GPU não disponível')\n",
        "\n",
        "# tf.test.gpu_device_name()\n",
        "# device_lib.list_local_devices()\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "path = 'I:/Meu Drive/dataset/'\n",
        "\n",
        "lista = os.listdir(path)\n",
        "\n",
        "print(lista)\n",
        "\n",
        "#neste ponto vamos contar todos os arquivos e mostrar a estrutura de subdiretórios\n",
        "totalFiles = 0\n",
        "totalDir = 0\n",
        "PATH = path\n",
        "\n",
        "for base, dirs, files in os.walk(PATH):\n",
        "    print('Searching in : ',base)\n",
        "    for directories in dirs:\n",
        "        totalDir += 1\n",
        "    for Files in files:\n",
        "        totalFiles += 1\n",
        "print('Total number of files',totalFiles)\n",
        "print('Total Number of directories',totalDir)\n",
        "print('Total:',(totalDir + totalFiles))\n",
        "\n",
        "#contamos as amostras de imagens para cada categoria.\n",
        "\n",
        "targets = ['nublado','chuva','sol']\n",
        "\n",
        "def count_targets(path_dataset, targets = targets):\n",
        "  \"\"\"\n",
        "  Função para contar amostras para cada categoria\n",
        "  entrada:\n",
        "    caminho para conjunto de dados - (exemplo, validação, treinamento ou teste)\n",
        "    alvos - alvos do conjunto de dados para cada categoria\n",
        "  saída:\n",
        "    alvo de contagem - matriz numpy com arquivos de contagem para cada categoria\n",
        "\n",
        "  \"\"\"\n",
        "  counts_target = []\n",
        "  for folder in targets:\n",
        "    path, dirs, files = next(os.walk(path_dataset+folder))\n",
        "    file_count = len(files)\n",
        "    counts_target.append([folder,file_count])\n",
        "\n",
        "  return np.array(counts_target, dtype = object)\n",
        "\n",
        "train_samples = count_targets(path_dataset = 'I:/Meu Drive/dataset/training/')\n",
        "validation_samples = count_targets(path_dataset = 'I:/Meu Drive/dataset/validation/')\n",
        "test_samples = count_targets(path_dataset = 'I:/Meu Drive/dataset/test/')\n",
        "\n",
        "\n",
        "#vamos plotar a distribuição de amostras para treinamento\n",
        "fig = plt.figure(figsize = (10,5))\n",
        "ax = sns.barplot(x = train_samples[:,0], y = train_samples[:,1])\n",
        "ax.set_title(\"Training Samples\")\n",
        "\n",
        "\n",
        "#vamos plotar a distribuição de amostras para validação\n",
        "fig = plt.figure(figsize = (10,5))\n",
        "ax = sns.barplot(x = train_samples[:,0], y = validation_samples[:,1])\n",
        "ax.set_title(\"Validation Samples\")\n",
        "\n",
        "\n",
        "#vamos plotar a distribuição de amostras para teste\n",
        "fig = plt.figure(figsize = (10,5))\n",
        "ax = sns.barplot(x = test_samples[:,0], y = test_samples[:,1])\n",
        "ax.set_title(\"Test Samples\")\n",
        "\n",
        "#Amostras de imagens\n",
        "\n",
        "#nublado\n",
        "img = Image.open(\"I:/Meu Drive/dataset/training/nublado/nublado_6.jpg\")\n",
        "print(img.size)\n",
        "print(plt.imshow(img))\n",
        "\n",
        "#chuva\n",
        "img = Image.open(\"I:/Meu Drive/dataset/training/chuva/chuva_1.jpg\")\n",
        "print(img.size)\n",
        "print(plt.imshow(img))\n",
        "\n",
        "#sol\n",
        "img = Image.open(\"I:/Meu Drive/dataset/training/sol/sol_1.jpg\")\n",
        "print(img.size)\n",
        "print(plt.imshow(img))\n",
        "\n",
        "\n",
        "#03 - Processamento de imagem e aumento de dados\n",
        "\n",
        "# mkdir(\"I:/Meu Drive/preview\")\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "# datagen = ImageDataGenerator(\n",
        "#         rotation_range=30,\n",
        "#         width_shift_range=0.2,\n",
        "#         height_shift_range=0.2,\n",
        "#         shear_range=0.2,\n",
        "#         zoom_range=0.2,\n",
        "#         horizontal_flip=True,\n",
        "#         fill_mode='nearest')\n",
        "\n",
        "# img = load_img('I:/Meu Drive/dataset/training/chuva/chuva_10.jpg')\n",
        "# x = img_to_array(img)\n",
        "# x = x.reshape((1,) + x.shape)\n",
        "\n",
        "# # o comando .flow() abaixo gera lotes de imagens transformadas aleatoriamente\n",
        "# # e salva os resultados no diretório `preview/`\n",
        "# i = 0\n",
        "# for batch in datagen.flow(x, batch_size=1,\n",
        "#                           save_to_dir='preview', save_prefix='rain', save_format='jpg'):\n",
        "#     i += 1\n",
        "#     if i > 20:\n",
        "#         break\n",
        "\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 150\n",
        "\n",
        "# esta é a configuração de aumento que usaremos apenas para treinamento\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "\n",
        "# esta é a configuração de aumento que usaremos para validação:\n",
        "# apenas para redimensionar\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# esta é a configuração de aumento que usaremos para testar:\n",
        "# apenas para redimensionar\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'I:/Meu Drive/dataset/training',\n",
        "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# este é um gerador semelhante, para dados de validação\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        'I:/Meu Drive/dataset/validation/',\n",
        "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical')\n",
        "\n",
        "# este é um gerador semelhante, para dados de teste\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        'I:/Meu Drive/dataset/test',\n",
        "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='categorical')\n",
        "\n",
        "\n",
        "#04 - Implementação do modelo\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "input_layer = keras.layers.Input(shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "\n",
        "#CNN - Convolutional Layers (Camadas Convolucionais)\n",
        "conv_layer = keras.layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(input_layer)\n",
        "conv_layer = keras.layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\")(conv_layer)\n",
        "pooling_layer = keras.layers.MaxPool2D(pool_size=(2,2))(conv_layer)\n",
        "\n",
        "conv_layer = keras.layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(pooling_layer)\n",
        "conv_layer = keras.layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\")(conv_layer)\n",
        "pooling_layer = keras.layers.MaxPool2D(pool_size=(2,2))(conv_layer)\n",
        "\n",
        "#FC - Fully connected (Totalmente conectado)\n",
        "flatten = keras.layers.Flatten()(pooling_layer)\n",
        "dense = keras.layers.Dense(200, activation=\"relu\")(flatten)\n",
        "dropout = keras.layers.Dropout(0.5)(dense)\n",
        "dense = keras.layers.Dense(100, activation=\"relu\")(dropout)\n",
        "dropout = keras.layers.Dropout(0.2)(dense)\n",
        "\n",
        "classifier = keras.layers.Dense(3, activation=\"softmax\")(dropout)\n",
        "\n",
        "model = keras.Model(inputs=input_layer, outputs=classifier)\n",
        "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "# if COLAB:\n",
        "#     display(SVG(model_to_dot(model, show_shapes=True,dpi=70).create(prog='dot', format='svg')))\n",
        "# else:\n",
        "#    display(SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg')))\n",
        "\n",
        "\n",
        "# pip install visualkeras (Instalar no VScode do PC)\n",
        "import visualkeras\n",
        "\n",
        "visualkeras.layered_view(model).show() # exibir usando o visualizador do sistema\n",
        "visualkeras.layered_view(model, legend=True,scale_xy=2)\n",
        "\n",
        "# 05 - Modelo de Treinamento\n",
        "history = model.fit(train_generator,validation_data=validation_generator, epochs=1300)\n",
        "\n",
        "# Salvando o modelo treinado\n",
        "model.save('I:/Meu Drive/model_cnn_save/hav_cnn_epochs_1300.h5')\n",
        "\n",
        "# 06 - Plotagem de Perda e Precisão\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "def plot_curves(history):\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Error')\n",
        "  plt.plot(history['loss'])\n",
        "  plt.plot(history['val_loss'])\n",
        "  plt.legend(['Training', 'Validation'])\n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.plot(history['accuracy'])\n",
        "  plt.plot(history['val_accuracy'])\n",
        "  plt.legend(['Training', 'Validation'], loc='lower right');\n",
        "\n",
        "\n",
        "# Desempenho do modelo de plotagem\n",
        "\n",
        "plot_curves(history.history)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 07 - Pontuações preliminares (treinamento e validação)\n",
        "\n",
        "scores_tr = model.evaluate(train_generator)\n",
        "print('Train loss    :', scores_tr[0])\n",
        "print('Train accuracy:', scores_tr[1])\n",
        "\n",
        "scores_val = model.evaluate(validation_generator)\n",
        "print('Val loss    :', scores_val[0])\n",
        "print('Val accuracy:', scores_val[1])\n",
        "\n",
        "scores_tr = model.evaluate(test_generator)\n",
        "print('Test loss    :', scores_tr[0])\n",
        "print('Test accuracy:', scores_tr[1])\n",
        "\n",
        "\n",
        "# 09 - Notas Finais (Teste)\n",
        "\n",
        "scores_tr = model.evaluate(test_generator)\n",
        "print('Test loss    :', scores_tr[0])\n",
        "print('Test accuracy:', scores_tr[1])\n",
        "\n",
        "\n",
        "# 10 - Previsões\n",
        "\n",
        "#imprimir índices das classes\n",
        "indices = train_generator.class_indices\n",
        "print(indices)\n",
        "\n",
        "\n",
        "#  Previsões usando o dataset\n",
        "\n",
        "\n",
        "#previsão para amostras de chuva\n",
        "path = 'I:/Meu Drive/dataset/test/chuva/chuva_346.jpg'\n",
        "img=load_img(path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
        "x=img_to_array(img)\n",
        "x=np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "classes = model.predict(images)\n",
        "plt.imshow(img)\n",
        "\n",
        "max_value = max(classes[0])\n",
        "max_index = list(classes[0]).index(max_value)\n",
        "\n",
        "np.around(classes[0], decimals = 2)\n",
        "print('Real: Chuva', ',Predicted:',list(indices.keys())[max_index])\n",
        "\n",
        "\n",
        "#previsão para amostras de nublado\n",
        "path = 'I:/Meu Drive/dataset/test/nublado/nublado_234.jpg'\n",
        "img=load_img(path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
        "x=img_to_array(img)\n",
        "x=np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "classes = model.predict(images)\n",
        "plt.imshow(img)\n",
        "\n",
        "max_value = max(classes[0])\n",
        "max_index = list(classes[0]).index(max_value)\n",
        "\n",
        "np.around(classes[0], decimals = 2)\n",
        "print('Real: Nublado', ',Predicted:',list(indices.keys())[max_index])\n",
        "\n",
        "\n",
        "#previsão para amostras do nascer do sol\n",
        "path = 'I:/Meu Drive/dataset/test/sol/sol_122.jpg'\n",
        "img=load_img(path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
        "x=img_to_array(img)\n",
        "x=np.expand_dims(x, axis=0)\n",
        "images = np.vstack([x])\n",
        "classes = model.predict(images)\n",
        "plt.imshow(img)\n",
        "\n",
        "max_value = max(classes[0])\n",
        "max_index = list(classes[0]).index(max_value)\n",
        "\n",
        "np.around(classes[0], decimals = 2)\n",
        "print('Real: Sol', ',Predicted:',list(indices.keys())[max_index])"
      ],
      "metadata": {
        "id": "1_iDh3yzJNYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}